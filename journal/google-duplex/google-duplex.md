---
title: Thoughts on Google Duplex
created: 10 May 2018
template: base
---
# Thoughts on Google Duplex

As I write this on the days following Google's announcement at IO 2018 of its prototype AI voice assistant, [Google Duplex](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html), which was demonstrated to navigate the complex skills of making a reservation over the phone speaking with a human who had no indication, nor suspicion, that they were talking to a machine (a feat which perhaps passes the [Turing test](https://en.wikipedia.org/wiki/Turing_test)), I'm struck by the lively debate this triggered. While in awe of the technological achievement, many vehemently question the morality of the act and raise alarms about the path this leads us on: a future where it's indistinguishable if a participant in a conversation is human or not. Why is that and issue?

Coincidentally, I'm currently reading the book [Sapiens, A Brief History of Humankind](http://www.ynharari.com/book/sapiens/) by Yuval Noah Harari. In it, I was surprised to find out about the existence of up to six human species in our evolutionary history, some of which overlapped in time. Yet us, Homo Sapiens, are the only ones left. Due in part to our adaptability, perhaps a more evolved instinct of collaboration, but also our ruthless primordial nature, we are the only human species left at the top of the power pyramid. All others died out or were exterminated. Anything not human, subjugated.

The history of humankind is a history of exerting dominance over its environment. And it think this last point is very important in understanding our reactions towards Google Duplex and AI in general.

We are not prepared to co-exist with another intelligent species.

With regards to AI, I find it telling that we're comfortable with bossing around an obvious simpleton of the AI kind like Apple's Siri, Microsoft's Cortana or Amazon's Alexa, yet we feel threatened by advancements which undermine our need to feel superior and in control. It's somehow acceptable to demand a machine do tasks for us. We may even sneer at its clumsiness. Yet when faced with the possibility of being spoken to by a machine without our express request, our primordial instincts kick in to see or foresee a threat.

In 2018 we're already surrounded by specialized AI that makes our modern lives possible, their existence invisible to us, in the software that controls utilities, financial markets, transportation, research and so forth.

But an AI entity which is capable of speaking to us so convincingly with complex natural language, a skill we consider to be so profoundly human and one that clearly differentiates us from other species, is forcing us to come to terms with our instinctual need to control something which we believe can threaten our absolute dominance of our world.

It's also telling that most movies about encounters with space-faring alien species center around the idea of humankind being invaded by a formidable foe.  I see it as merely a reflection of the fears stemming from our own violent history of subjugation of other people we've encountered through our colonial endeavours.

This idea from the book Sapiens also stuck with me: since the Agricultural Revolution some 10,000 years ago when humans transitioned from small groups of nomadic hunter-gatherers to self-organizing in larger stable societies, not enough time has passed for humans to evolve an instinct of mass-collaboration which would enable us thrive as a species together. At the core of our accelerated advancement is competition, most of the time amongst ourselves as individuals, sometimes as groups.

With the exponential advancement of AI and its eventual achievement of [superintelligence](https://en.wikipedia.org/wiki/Superintelligence), humankind is faced with the prospect of competing again. This time with an entity of our own creation.

We are still not prepared to co-exist with another intelligent species.
