---
title: Thoughts on Google Duplex
created: 10 May 2018
template: base
---
# Thoughts on Google Duplex

As I write this in the aftermath of Google's announcement at IO 2018 of its prototype AI voice assistant, [Google Duplex](https://ai.googleblog.com/2018/05/duplex-ai-system-for-natural-conversation.html), which was demonstrated to navigate the complex skills of making a reservation over the phone by speaking with a human who had no indication nor suspicion that they were talking to a machine (a feat which perhaps passes the [Turing test](https://en.wikipedia.org/wiki/Turing_test)), I'm struck by the lively debate this triggered. While in awe of the technological achievement, many vehemently question the morality of the act and raise alarms about the path this leads us on: a future where it's indistinguishable if a participant in a conversation is human or not. Why is that an issue?

Coincidentally, I'm currently reading the book [Sapiens, A Brief History of Humankind](http://www.ynharari.com/book/sapiens/) by Yuval Noah Harari. In it, I learned about the existence of at least six human species in our evolutionary history, some of which overlapped for long periods of time. Yet us, Homo Sapiens, are the only ones left. Due in part to our adaptability, perhaps a more evolved instinct of collaboration, but also our ruthless primordial nature, we are the only human species left at the top of the power pyramid. All others died out or were exterminated. Anything not human, subjugated.

The history of humankind is a history of exerting dominance over its environment. And it think this last point is very important in understanding our reactions towards Google Duplex and AI in general.

We are not prepared to co-exist with another intelligent species.

With regards to AI, I find it telling that we're comfortable with bossing around an obvious simpleton of the AI kind, like Apple's Siri, Microsoft's Cortana or Amazon's Alexa, yet we feel threatened by advancements which undermine our desire for superiority and control. It's somehow acceptable to demand a machine do tasks for us. We may even sneer at its clumsiness. Yet when faced with the possibility of being even spoken to by a machine without our express request, our instincts kick in to foresee a threat.

In 2018 we're already surrounded by specialized AI that makes our modern life possible, its existence invisible to us, in the software that controls utilities, financial markets, transportation, research and so forth.

But an AI entity which is capable of speaking to us so convincingly with complex natural language, a skill we hold to be so profoundly human and one that clearly differentiates us from other species, is forcing us to come to terms with our instinctual need to control something which we believe can threaten our absolute dominance of the world.

It's also telling that most movies about encounters with spacefaring alien species center around the idea of humankind being invaded by a formidable foe.  I see it as a reflection of the fears stemming from our own violent history of subjugating other people we've encountered through our colonial endeavours.

This idea from the book Sapiens also stuck with me: since the Agricultural Revolution some 10,000 years ago when humans transitioned from small groups of nomadic hunter-gatherers to self-organizing in larger stable societies, not enough time has passed for humans to evolve an instinct of mass-collaboration which would enable us thrive together as a species. Competition is at the core of our accelerated advancement, most of the time amongst ourselves as individuals, sometimes as groups.

With the exponential advancement of AI and its eventual achievement of [superintelligence](https://en.wikipedia.org/wiki/Superintelligence), humankind is faced with the prospect of competing again. This time with an entity of our own creation.

We are still not prepared to co-exist with another intelligent species.
